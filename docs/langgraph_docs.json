[
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/cloud/quick_start/#quick-start)\n\n# Quick Start [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#quick-start \"Permanent link\")\n\nThis quick start guide will cover how to build a simple agent that can look up things on the internet. We will then deploy it to LangGraph Cloud, use the LangGraph Studio to visualize and test it out, and use the LangGraph SDK to interact with it.\n\n## Set up requirements [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#set-up-requirements \"Permanent link\")\n\nThis tutorial will use:\n\n- Anthropic for the LLM - sign up and get an API key [here](https://console.anthropic.com/)\n- Tavily for the search engine - sign up and get an API key [here](https://app.tavily.com/)\n- LangSmith for hosting - sign up and get an API key [here](https://smith.langchain.com/)\n\n## Set up local files [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#set-up-local-files \"Permanent link\")\n\n1. Create a new application with the following directory and files:\n\n[Python](#__tabbed_1_1)[Javascript](#__tabbed_1_2)\n\n```\n<my-app>/\n|-- agent.py            # code for your LangGraph agent\n|-- requirements.txt    # Python packages required for your graph\n|-- langgraph.json      # configuration file for LangGraph\n|-- .env                # environment files with API keys\n\n```\n\n```\n<my-app>/\n|-- agent.ts            # code for your LangGraph agent\n|-- package.json        # Javascript packages required for your graph\n|-- langgraph.json      # configuration file for LangGraph\n|-- .env                # environment files with API keys\n\n```\n\n1. The `agent.py`/ `agent.ts` file should contain code for defining your graph. The following code is a simple example, the important thing is that at some point in your file you compile your graph and assign the compiled graph to a variable (in this case the `graph` variable). This example code uses `create_react_agent`, a prebuilt agent. You can read more about it [here](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#react-implementation).\n\n[Python](#__tabbed_2_1)[Javascript](#__tabbed_2_2)\n\n```md-code__content\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langgraph.prebuilt import create_react_agent\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n\ntools = [TavilySearchResults(max_results=2)]\n\ngraph = create_react_agent(model, tools)\n\n```\n\n```md-code__content\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-20240620\",\n});\n\nconst tools = [\\\n  new TavilySearchResults({ maxResults: 3, }),\\\n];\n\nexport const graph = createReactAgent({ llm: model, tools });\n\n```\n\n1. The `requirements.txt`/ `package.json` file should contain any dependencies for your graph(s). In this case we only require four packages for our graph to run:\n\n[Python](#__tabbed_3_1)[Javascript](#__tabbed_3_2)\n\n```md-code__content\nlanggraph\nlangchain_anthropic\ntavily-python\nlangchain_community\n\n```\n\n```md-code__content\n{\n  \"name\": \"my-app\",\n  \"packageManager\": \"yarn@1.22.22\",\n  \"dependencies\": {\n    \"@langchain/community\": \"^0.2.31\",\n    \"@langchain/core\": \"^0.2.31\",\n    \"@langchain/langgraph\": \"0.2.0\",\n    \"@langchain/openai\": \"^0.2.8\"\n  }\n}\n\n```\n\n1. The [`langgraph.json`](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) file is a configuration file that describes what graph(s) you are going to host. In this case we only have one graph to host: the compiled `graph` object from `agent.py`/ `agent.ts`.\n\n[Python](#__tabbed_4_1)[Javascript](#__tabbed_4_2)\n\n```md-code__content\n{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"agent\": \"./agent.py:graph\"\n  },\n  \"env\": \".env\"\n}\n\n```\n\n```md-code__content\n{\n  \"node_version\": \"20\",\n  \"dockerfile_lines\": [],\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"agent\": \"./src/agent.ts:graph\"\n  },\n  \"env\": \".env\"\n}\n\n```\n\nLearn more about the LangGraph CLI configuration file [here](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file).\n\n1. The `.env` file should have any environment variables needed to run your graph. This will only be used for local testing, so if you are not testing locally you can skip this step. NOTE: if you do add this, you should NOT check this into git. For this graph, we need two environment variables:\n\n\n\n```md-code__content\nANTHROPIC_API_KEY=...\nTAVILY_API_KEY=...\n\n```\n\n\nNow that we have set everything up on our local file system, we are ready to host our graph.\n\n## Test the graph build locally [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#test-the-graph-build-locally \"Permanent link\")\n\n### Using LangGraph Studio Desktop (recommended) [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#using-langgraph-studio-desktop-recommended \"Permanent link\")\n\n![LangGraph Studio Desktop](https://langchain-ai.github.io/langgraph/cloud/img/graph_video_poster.png)\n\nTesting your graph locally is easy with LangGraph Studio Desktop. LangGraph Studio offers a new way to develop LLM applications by providing a specialized agent IDE that enables visualization, interaction, and debugging of complex agentic applications\n\nWith visual graphs and the ability to edit state, you can better understand agent workflows and iterate faster. LangGraph Studio integrates with [LangSmith](https://smith.langchain.com) so you can collaborate with teammates to debug failure modes.\n\n### Using the LangGraph CLI [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#using-the-langgraph-cli \"Permanent link\")\n\nBefore deploying to the cloud, we probably want to test the building of our graph locally. This is useful to make sure we have configured our [CLI configuration file](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) correctly and our graph runs.\n\nIn order to do this we can first install the LangGraph CLI\n\n```md-code__content\npip install langgraph-cli\n\n```\n\nWe can then test our API server locally. This requires access to LangGraph closed beta. In order to run the server locally, you will need to add your `LANGSMITH_API_KEY` to the .env file so we can validate you have access to LangGraph closed beta.\n\n```md-code__content\nlanggraph up\n\n```\n\nThis will start up the LangGraph API server locally. If this runs successfully, you should see something like:\n\n```md-code__content\nReady!\n- API: http://localhost:8123\n2024-06-26 19:20:41,056:INFO:uvicorn.access 127.0.0.1:44138 - \"GET /ok HTTP/1.1\" 200\n\n```\n\nYou can now test this out! **Note: this local server is intended SOLELY for local testing purposes and is not performant enough for production applications, so please do not use it as such.** To test it out, you can go to another terminal window and run:\n\n```md-code__content\ncurl --request POST \\\n    --url http://localhost:8123/runs/stream \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n    \"assistant_id\": \"agent\",\n    \"input\": {\n        \"messages\": [\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": \"How are you?\"\\\n            }\\\n        ]\n    },\n    \"metadata\": {},\n    \"config\": {\n        \"configurable\": {}\n    },\n    \"multitask_strategy\": \"reject\",\n    \"stream_mode\": [\\\n        \"values\"\\\n    ]\n}'\n\n```\n\nIf you get back a valid response, then all is functioning properly!\n\n## Deploy to Cloud [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#deploy-to-cloud \"Permanent link\")\n\n### Push your code to GitHub [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#push-your-code-to-github \"Permanent link\")\n\nTurn the `<my-app>` directory into a GitHub repo. You can use the GitHub CLI if you like, or just create a repo manually (if unfamiliar, instructions [here](https://docs.github.com/en/migrations/importing-source-code/using-the-command-line-to-import-source-code/adding-locally-hosted-code-to-github)).\n\n### Deploy from GitHub with LangGraph Cloud [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#deploy-from-github-with-langgraph-cloud \"Permanent link\")\n\nOnce you have created your github repository with a Python file containing your compiled graph as well as a `langgraph.json` file containing the configuration for hosting your graph, you can head over to LangSmith and click on the 🚀 icon on the left navbar to create a new deployment. Then click the `+ New Deployment` button.\n\n![Langsmith Workflow](https://langchain-ai.github.io/langgraph/cloud/img/cloud_deployment.png)\n\n**_If you have not deployed to LangGraph Cloud before:_** there will be a button that shows up saying Import from GitHub. You’ll need to follow that flow to connect LangGraph Cloud to GitHub.\n\n**_Once you have set up your GitHub connection:_** the new deployment page will look as follows:\n\n![Deployment before being filled out](https://langchain-ai.github.io/langgraph/cloud/deployment/img/deployment_page.png)\n\nTo deploy your application, you should do the following:\n\n1. Select your GitHub username or organization from the selector\n2. Search for your repo to deploy in the search bar and select it\n3. Choose any name\n4. In the `LangGraph API config file` field, enter the path to your `langgraph.json` file (which in this case is just `langgraph.json`)\n5. For Git Reference, you can select either the git branch for the code you want to deploy, or the exact commit SHA.\n6. If your chain relies on environment variables, add those in. They will be propagated to the underlying server so your code can access them. In this case, we need `ANTHROPIC_API_KEY` and `TAVILY_API_KEY`.\n\nPutting this all together, you should have something as follows for your deployment details:\n\n![Deployment filled out](https://langchain-ai.github.io/langgraph/cloud/deployment/img/deploy_filled_out.png)\n\nHit `Submit` and your application will start deploying!\n\n## Inspect Traces + Monitor Service [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#inspect-traces-monitor-service \"Permanent link\")\n\n### Deployments View [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#deployments-view \"Permanent link\")\n\nAfter your deployment is complete, your deployments page should look as follows:\n\n![Deployed page](https://langchain-ai.github.io/langgraph/cloud/deployment/img/deployed_page.png)\n\nYou can see that by default, you get access to the `Trace Count` monitoring chart and `Recent Traces` run view. These are powered by LangSmith.\n\nYou can click on `All Charts` to view all monitoring info for your server, or click on `See tracing project` to get more information on an individual trace.\n\n### Access the Docs [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#access-the-docs \"Permanent link\")\n\nYou can access the docs by clicking on the API docs link, which should send you to a page that looks like this:\n\n![API Docs page](https://langchain-ai.github.io/langgraph/cloud/deployment/img/api_page.png)\n\nYou won’t actually be able to test any of the API endpoints without authorizing first. To do so, grab your Langsmith API key and add it at the top where it says `API KEY (X-API-KEY)`. You should now be able to select any of the API endpoints, click `Test Request`, enter the parameters you would like to pass, and then click `Send` to view the results of the API call.\n\n## Interact with your deployment via LangGraph Studio [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#interact-with-your-deployment-via-langgraph-studio \"Permanent link\")\n\nIf you click on your deployment you should see a blue button in the top right that says `LangGraph Studio`. Clicking on this button will take you to a page that looks like this:\n\n![Studio UI before being run](https://langchain-ai.github.io/langgraph/cloud/deployment/img/graph_visualization.png)\n\nOn this page you can test out your graph by passing in starting states and clicking `Start Run` (this should behave identically to calling `.invoke`). You will then be able to look into the execution thread for each run and explore the steps your graph is taking to produce its output.\n\n![Studio UI once being run](https://langchain-ai.github.io/langgraph/cloud/deployment/img/graph_run.png)\n\n## Use with the SDK [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#use-with-the-sdk \"Permanent link\")\n\nOnce you have tested that your hosted graph works as expected using LangGraph Studio, you can start using your hosted graph all over your organization by using the LangGraph SDK. Let's see how we can access our hosted graph and execute our run from a python file.\n\nFirst, make sure you have the SDK installed by calling `pip install langgraph_sdk`.\n\nBefore using, you need to get the URL of your LangGraph deployment. You can find this in the `Deployment` view. Click the URL to copy it to the clipboard.\n\nYou also need to make sure you have set up your API key properly so you can authenticate with LangGraph Cloud.\n\n```md-code__content\nexport LANGSMITH_API_KEY=...\n\n```\n\nThe first thing to do when using the SDK is to setup our client, access our assistant, and create a thread to execute a run on:\n\n[Python](#__tabbed_5_1)[Javascript](#__tabbed_5_2)[CURL](#__tabbed_5_3)\n\n```md-code__content\nfrom langgraph_sdk import get_client\n\nclient = get_client(url=<DEPLOYMENT_URL>)\n# get default assistant\nassistants = await client.assistants.search()\nassistant = [a for a in assistants if not a[\"config\"]][0]\n# create thread\nthread = await client.threads.create()\nprint(thread)\n\n```\n\n```md-code__content\nimport { Client } from \"@langchain/langgraph-sdk\";\n\nconst client = new Client({ apiUrl: <DEPLOYMENT_URL> });\n// get default assistant\nconst assistants = await client.assistants.search();\nconst assistant = assistants.find(a => !a.config);\n// create thread\nconst thread = await client.threads.create();\nconsole.log(thread)\n\n```\n\n```md-code__content\ncurl --request POST \\\n    --url <DEPLOYMENT_URL>/assistants/search \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n        \"limit\": 10,\n        \"offset\": 0\n    }' | jq -c 'map(select(.config == null or .config == {})) | .[0]' && \\\ncurl --request POST \\\n    --url <DEPLOYMENT_URL>/threads \\\n    --header 'Content-Type: application/json' \\\n    --data '{}'\n\n```\n\nWe can then execute a run on the thread:\n\n[Python](#__tabbed_6_1)[Javascript](#__tabbed_6_2)[CURL](#__tabbed_6_3)\n\n```md-code__content\ninput = {\"messages\":[{\"role\": \"user\", \"content\": \"Hello! My name is Bagatur and I am 26 years old.\"}]}\n\nasync for chunk in client.runs.stream(\n        thread['thread_id'],\n        assistant[\"assistant_id\"],\n        input=input,\n        stream_mode=\"updates\",\n    ):\n    if chunk.data and chunk.event != \"metadata\":\n        print(chunk.data)\n\n```\n\n```md-code__content\nconst input = { \"messages\":[{ \"role\": \"user\", \"content\": \"Hello! My name is Bagatur and I am 26 years old.\" }] };\n\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistant[\"assistant_id\"],\n  {\n    input,\n  }\n);\nfor await (const chunk of streamResponse) {\n  if (chunk.data && chunk.event !== \"metadata\" ) {\n    console.log(chunk.data);\n  }\n}\n\n```\n\n```md-code__content\ncurl --request POST \\\n  --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n  --header 'Content-Type: application/json' \\\n  --data \"{\n    \\\"assistant_id\\\": <ASSISTANT_ID>,\n    \\\"input\\\": {\\\"messages\\\": [{\\\"role\\\": \\\"human\\\", \\\"content\\\": \\\"Hello! My name is Bagatur and I am 26 years old.\\\"}]},\n  }\" | sed 's/\\r$//' | awk '\n  /^event:/ { event = $2 }\n  /^data:/ {\n      json_data = substr($0, index($0, $2))\n\n      if (event != \"metadata\") {\n      print json_data\n      }\n  }'\n\n```\n\nOutput:\n\n```\n{'agent': {'messages': [{'content': \"Hi Bagatur! It's nice to meet you. How can I assist you today?\", 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_9cb5d38cf7'}, 'type': 'ai', 'name': None, 'id': 'run-c89118b7-1b1e-42b9-a85d-c43fe99881cd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\n```\n\n## What's Next [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#whats-next \"Permanent link\")\n\nCongratulations! If you've worked your way through this tutorial you are well on your way to becoming a LangGraph Cloud expert. Here are some other resources to check out to help you out on the path to expertise:\n\n### LangGraph Cloud How-tos [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#langgraph-cloud-how-tos \"Permanent link\")\n\nIf you want to learn more about streaming from hosted graphs, check out the Streaming [how-to guides](https://langchain-ai.github.io/langgraph/cloud/how-tos/#streaming).\n\nTo learn more about double-texting and all the ways you can handle it in your application, read up on these [how-to guides](https://langchain-ai.github.io/langgraph/cloud/how-tos/#double-texting).\n\nTo learn about how to include different human-in-the-loop behavior in your graph, take a look at [these how-tos](https://langchain-ai.github.io/langgraph/cloud/how-tos/#human-in-the-loop).\n\n### LangGraph Tutorials [¶](https://langchain-ai.github.io/langgraph/cloud/quick_start/\\#langgraph-tutorials \"Permanent link\")\n\nBefore hosting, you have to write a graph to host. Here are some tutorials to get you more comfortable with writing LangGraph graphs and give you inspiration for the types of graphs you want to host.\n\n[This tutorial](https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/) walks you through how to write a customer support bot using LangGraph.\n\nIf you are interested in writing a SQL agent, check out [this tutorial](https://langchain-ai.github.io/langgraph/tutorials/sql-agent/).\n\nCheck out the [LangGraph tutorials](https://langchain-ai.github.io/langgraph/tutorials/) page to read about more exciting use cases.\n\n## Comments\n\nBack to top",
    "metadata": {
      "title": "Quick Start",
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/cloud/quick_start/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  },
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_javascript/#how-to-set-up-a-langgraphjs-application-for-deployment)\n\n# How to Set Up a LangGraph.js Application for Deployment [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_javascript/\\#how-to-set-up-a-langgraphjs-application-for-deployment \"Permanent link\")\n\nA [LangGraph.js](https://langchain-ai.github.io/langgraphjs/) application must be configured with a [LangGraph API configuration file](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) in order to be deployed to LangGraph Cloud (or to be self-hosted). This how-to guide discusses the basic steps to setup a LangGraph.js application for deployment using `package.json` to specify project dependencies.\n\nThis walkthrough is based on [this repository](https://github.com/langchain-ai/langgraphjs-studio-starter), which you can play around with to learn more about how to setup your LangGraph application for deployment.\n\nThe final repo structure will look something like this:\n\n```md-code__content\nmy-app/\n├── src # all project code lies within here\n│   ├── utils # optional utilities for your graph\n│   │   ├── tools.ts # tools for your graph\n│   │   ├── nodes.ts # node functions for you graph\n│   │   └── state.ts # state definition of your graph\n│   └── agent.ts # code for constructing your graph\n├── package.json # package dependencies\n├── .env # environment variables\n└── langgraph.json # configuration file for LangGraph\n\n```\n\nAfter each step, an example file directory is provided to demonstrate how code can be organized.\n\n## Specify Dependencies [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_javascript/\\#specify-dependencies \"Permanent link\")\n\nDependencies can be specified in a `package.json`. If none of these files is created, then dependencies can be specified later in the [LangGraph API configuration file](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_javascript/#create-langgraph-api-config).\n\nExample `package.json` file:\n\n```md-code__content\n{\n  \"name\": \"langgraphjs-studio-starter\",\n  \"packageManager\": \"yarn@1.22.22\",\n  \"dependencies\": {\n    \"@langchain/community\": \"^0.2.31\",\n    \"@langchain/core\": \"^0.2.31\",\n    \"@langchain/langgraph\": \"^0.2.0\",\n    \"@langchain/openai\": \"^0.2.8\"\n  }\n}\n\n```\n\nExample file directory:\n\n```md-code__content\nmy-app/\n└── package.json # package dependencies\n\n```\n\n## Specify Environment Variables [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_javascript/\\#specify-environment-variables \"Permanent link\")\n\nEnvironment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](https://langchain-ai.github.io/langgraph/cloud/reference/env_var/) to configure additional variables for a deployment.\n\nExample `.env` file:\n\n```md-code__content\nMY_ENV_VAR_1=foo\nMY_ENV_VAR_2=bar\nOPENAI_API_KEY=key\nTAVILY_API_KEY=key_2\n\n```\n\nExample file directory:\n\n```md-code__content\nmy-app/\n├── package.json\n└── .env # environment variables\n\n```\n\n## Define Graphs [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_javascript/\\#define-graphs \"Permanent link\")\n\nImplement your graphs! Graphs can be defined in a single file or multiple files. Make note of the variable names of each compiled graph to be included in the LangGraph application. The variable names will be used later when creating the [LangGraph API configuration file](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file).\n\nHere is an example `agent.ts`:\n\n```md-code__content\nimport type { AIMessage } from \"@langchain/core/messages\";\nimport { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nimport { MessagesAnnotation, StateGraph } from \"@langchain/langgraph\";\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\n\nconst tools = [\\\n  new TavilySearchResults({ maxResults: 3, }),\\\n];\n\n// Define the function that calls the model\nasync function callModel(\n  state: typeof MessagesAnnotation.State,\n) {\n  /**\n   * Call the LLM powering our agent.\n   * Feel free to customize the prompt, model, and other logic!\n   */\n  const model = new ChatOpenAI({\n    model: \"gpt-4o\",\n  }).bindTools(tools);\n\n  const response = await model.invoke([\\\n    {\\\n      role: \"system\",\\\n      content: `You are a helpful assistant. The current date is ${new Date().getTime()}.`\\\n    },\\\n    ...state.messages\\\n  ]);\n\n  // MessagesAnnotation supports returning a single message or array of messages\n  return { messages: response };\n}\n\n// Define the function that determines whether to continue or not\nfunction routeModelOutput(state: typeof MessagesAnnotation.State) {\n  const messages = state.messages;\n  const lastMessage: AIMessage = messages[messages.length - 1];\n  // If the LLM is invoking tools, route there.\n  if ((lastMessage?.tool_calls?.length ?? 0) > 0) {\n    return \"tools\";\n  }\n  // Otherwise end the graph.\n  return \"__end__\";\n}\n\n// Define a new graph.\n// See https://langchain-ai.github.io/langgraphjs/how-tos/define-state/#getting-started for\n// more on defining custom graph states.\nconst workflow = new StateGraph(MessagesAnnotation)\n  // Define the two nodes we will cycle between\n  .addNode(\"callModel\", callModel)\n  .addNode(\"tools\", new ToolNode(tools))\n  // Set the entrypoint as `callModel`\n  // This means that this node is the first one called\n  .addEdge(\"__start__\", \"callModel\")\n  .addConditionalEdges(\n    // First, we define the edges' source node. We use `callModel`.\n    // This means these are the edges taken after the `callModel` node is called.\n    \"callModel\",\n    // Next, we pass in the function that will determine the sink node(s), which\n    // will be called after the source node is called.\n    routeModelOutput,\n    // List of the possible destinations the conditional edge can route to.\n    // Required for conditional edges to properly render the graph in Studio\n    [\\\n      \"tools\",\\\n      \"__end__\"\\\n    ],\n  )\n  // This means that after `tools` is called, `callModel` node is called next.\n  .addEdge(\"tools\", \"callModel\");\n\n// Finally, we compile it!\n// This compiles it into a graph you can invoke and deploy.\nexport const graph = workflow.compile();\n\n```\n\nAssign `CompiledGraph` to Variable\n\nThe build process for LangGraph Cloud requires that the `CompiledGraph` object be assigned to a variable at the top-level of a JavaScript module (alternatively, you can provide [a function that creates a graph](https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/)).\n\nExample file directory:\n\n```md-code__content\nmy-app/\n├── src # all project code lies within here\n│   ├── utils # optional utilities for your graph\n│   │   ├── tools.ts # tools for your graph\n│   │   ├── nodes.ts # node functions for you graph\n│   │   └── state.ts # state definition of your graph\n│   └── agent.ts # code for constructing your graph\n├── package.json # package dependencies\n├── .env # environment variables\n└── langgraph.json # configuration file for LangGraph\n\n```\n\n## Create LangGraph API Config [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_javascript/\\#create-langgraph-api-config \"Permanent link\")\n\nCreate a [LangGraph API configuration file](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) called `langgraph.json`. See the [LangGraph CLI reference](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) for detailed explanations of each key in the JSON object of the configuration file.\n\nExample `langgraph.json` file:\n\n```md-code__content\n{\n  \"node_version\": \"20\",\n  \"dockerfile_lines\": [],\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"agent\": \"./src/agent.ts:graph\"\n  },\n  \"env\": \".env\"\n}\n\n```\n\nNote that the variable name of the `CompiledGraph` appears at the end of the value of each subkey in the top-level `graphs` key (i.e. `:<variable_name>`).\n\nConfiguration Location\n\nThe LangGraph API configuration file must be placed in a directory that is at the same level or higher than the TypeScript files that contain compiled graphs and associated dependencies.\n\n## Next [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_javascript/\\#next \"Permanent link\")\n\nAfter you setup your project and place it in a github repo, it's time to [deploy your app](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/).\n\n## Comments\n\nBack to top",
    "metadata": {
      "title": "Setup App (JavaScript)",
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/cloud/deployment/setup_javascript/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  },
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/cloud/#langgraph-cloud-beta)\n\n# LangGraph Cloud (beta) [¶](https://langchain-ai.github.io/langgraph/cloud/\\#langgraph-cloud-beta \"Permanent link\")\n\nTip\n\n- LangGraph is an MIT-licensed open-source library, which we are committed to maintaining and growing for the community.\n- LangGraph Cloud is an optional managed hosting service for LangGraph, which provides additional features geared towards production deployments.\n- We are actively contributing improvements back to LangGraph informed by our work on LangGraph Cloud.\n- You can always deploy LangGraph applications on your own infrastructure using the open-source LangGraph project.\n\nUnder Construction\n\nLangGraph Cloud documentation is under construction. Contents may change until general availability.\n\n## Overview [¶](https://langchain-ai.github.io/langgraph/cloud/\\#overview \"Permanent link\")\n\nLangGraph Cloud is a managed service for deploying and hosting LangGraph applications. Deploying applications with LangGraph Cloud shortens the time-to-market for developers. With one click, deploy a production-ready API with built-in persistence for your LangGraph application. LangGraph Cloud APIs are horizontally scalable and deployed with durable storage.\n\nThe LangGraph Cloud API exposes functionality of your LangGraph application through [Assistants](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#assistants). An assistant abstracts the cognitive architecture of your graph. Invoke an assistant by calling the pre-built [API endpoints](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/).\n\nLangGraph Cloud is seamlessly integrated with [LangSmith](https://www.langchain.com/langsmith) and is accessible from within the LangSmith UI.\n\nLangGraph Cloud applications can be tested and debugged using the [LangGraph Studio Desktop](https://github.com/langchain-ai/langgraph-studio).\n\n## Key Features [¶](https://langchain-ai.github.io/langgraph/cloud/\\#key-features \"Permanent link\")\n\nThe LangGraph Cloud API supports key LangGraph features in addition to new functionality for enabling complex, agentic workflows.\n\n- **Assistants and Threads**: Assistants abstract the cognitive architecture of graphs and threads track the state/history of graphs.\n- **Streaming**: API support for [LangGraph streaming modes](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming) including setting multiple streaming modes at the same time.\n- **Human-in-the-Loop**: API support for [LangGraph human-in-the-loop features](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop).\n- **Double Texting**: Configure how assistants respond when new input is received while processing a previous input. Interrupt, rollback, reject, or enqueue.\n- **Background Runs/Cron Jobs**: A built-in task queue enables background runs and scheduled cron jobs.\n- **Stateless Runs**: For simpler use cases, invoke an assistant without needing to create a thread.\n\n## Documentation [¶](https://langchain-ai.github.io/langgraph/cloud/\\#documentation \"Permanent link\")\n\n- [Tutorials](https://langchain-ai.github.io/langgraph/cloud/quick_start/): Learn to build and deploy applications for LangGraph Cloud.\n- [How-to Guides](https://langchain-ai.github.io/langgraph/cloud/how-tos/): Learn how to set up a LangGraph application for deployment and implement features of the LangGraph Cloud API such as streaming tokens, configuring double texting, and creating cron jobs. Go here if you want to copy and run a specific code snippet.\n- [Conceptual Guides](https://langchain-ai.github.io/langgraph/cloud/concepts/api/): In-depth explanations of the core data models (e.g. assistants), key features of the LangGraph Cloud API (e.g. double texting), and the architecture of a LangGraph Cloud deployment.\n- [Reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/): References for the LangGraph Cloud API, the corresponding Python and JS/TS SDKs, the LangGraph CLI, and deployment environment variables.\n\n## Comments\n\nBack to top",
    "metadata": {
      "title": "LangGraph Cloud (beta)",
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/cloud/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  },
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/cloud/deployment/custom_docker/#how-to-customize-dockerfile)\n\n# How to customize Dockerfile [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/custom_docker/\\#how-to-customize-dockerfile \"Permanent link\")\n\nUsers can add an array of additional lines to add to the Dockerfile following the import from the parent LangGraph image. In order to do this, you simply need to modify your `langgraph.json` file by passing in the commands you want run to the `dockerfile_lines` key. For example, if we wanted to use `Pillow` in our graph you would need to add the following dependencies:\n\n```md-code__content\n{\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"openai_agent\": \"./openai_agent.py:agent\",\n    },\n    \"env\": \"./.env\",\n    \"dockerfile_lines\": [\\\n        \"RUN apt-get update && apt-get install -y libjpeg-dev zlib1g-dev libpng-dev\",\\\n        \"RUN pip install Pillow\"\\\n    ]\n}\n\n```\n\nThis would install the system packages required to use Pillow if we were working with `jpeq` or `png` image formats.\n\n## Comments\n\nBack to top",
    "metadata": {
      "title": "Customize Dockerfile",
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/cloud/deployment/custom_docker/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  },
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/#how-to-deploy-to-langgraph-cloud)\n\n# How to Deploy to LangGraph Cloud [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/\\#how-to-deploy-to-langgraph-cloud \"Permanent link\")\n\nLangGraph Cloud is available within [LangSmith](https://www.langchain.com/langsmith). To deploy a LangGraph Cloud API, navigate to the [LangSmith UI](https://smith.langchain.com/).\n\n## Prerequisites [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/\\#prerequisites \"Permanent link\")\n\n1. LangGraph Cloud applications are deployed from GitHub repositories. Configure and upload a LangGraph Cloud application to a GitHub repository in order to deploy it to LangGraph Cloud.\n2. [Verify that the LangGraph API runs locally](https://langchain-ai.github.io/langgraph/cloud/deployment/test_locally/). If the API does not build and run successfully (i.e. `langgraph up`), deploying to LangGraph Cloud will fail as well.\n\n## Create New Deployment [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/\\#create-new-deployment \"Permanent link\")\n\nStarting from the [LangSmith UI](https://smith.langchain.com/)...\n\n1. In the left-hand navigation panel, select `Deployments`. The `Deployments` view contains a list of existing LangGraph Cloud deployments.\n2. In the top-right corner, select `+ New Deployment` to create a new deployment.\n3. In the `Create New Deployment` panel, fill out the required fields.\n1. `Deployment details`\n      1. Select `Import from GitHub` and follow the GitHub OAuth workflow to install and authorize LangChain's `hosted-langserve` GitHub app to access the selected repositories. After installation is complete, return to the `Create New Deployment` panel and select the GitHub repository to deploy from the dropdown menu.\n      2. Specify a name for the deployment.\n      3. Specify the desired `Git Branch`. A deployment is linked to a branch. When a new revision is created, code for the linked branch will be deployed. The branch can be updated later in the [Deployment Settings](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/#deployment-settings).\n      4. Specify the full path to the [LangGraph API config file](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) including the file name. For example, if the file `langgraph.json` is in the root of the repository, simply specify `langgraph.json`.\n      5. Check/uncheck checkbox to `Automatically update deployment on push to branch`. If checked, the deployment will automatically be updated when changes are pushed to the specified `Git Branch`. This setting can be enabled/disabled later in the [Deployment Settings](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/#deployment-settings).\n2. Select the desired `Deployment Type`.\n      1. `Development` deployments are meant for non-production use cases and are provisioned with minimal resources.\n      2. `Production` deployments can serve up to 500 requests/second and are provisioned with highly available storage with automatic backups.\n3. Determine if the deployment should be `Shareable through LangGraph Studio`.\n      1. If unchecked, the deployment will only be accessible with a valid LangSmith API key for the workspace.\n      2. If checked, the deployment will be accessible through LangGraph Studio to any LangSmith user. A direct URL to LangGraph Studio for the deployment will be provided to share with other LangSmith users.\n4. Specify `Environment Variables` and secrets. See the [Environment Variables reference](https://langchain-ai.github.io/langgraph/cloud/reference/env_var/) to configure additional variables for the deployment.\n      1. Sensitive values such as API keys (e.g. `OPENAI_API_KEY`) should be specified as secrets.\n      2. Additional non-secret environment variables can be specified as well.\n5. A new LangSmith `Tracing Project` is automatically created with the same name as the deployment.\n4. In the top-right corner, select `Submit`. After a few seconds, the `Deployment` view appears and the new deployment will be queued for provisioning.\n\n## Create New Revision [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/\\#create-new-revision \"Permanent link\")\n\nWhen [creating a new deployment](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/#create-new-deployment), a new revision is created by default. Subsequent revisions can be created to deploy new code changes.\n\nStarting from the [LangSmith UI](https://smith.langchain.com/)...\n\n1. In the left-hand navigation panel, select `Deployments`. The `Deployments` view contains a list of existing LangGraph Cloud deployments.\n2. Select an existing deployment to create a new revision for.\n3. In the `Deployment` view, in the top-right corner, select `+ New Revision`.\n4. In the `New Revision` modal, fill out the required fields.\n1. Specify the full path to the [LangGraph API config file](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) including the file name. For example, if the file `langgraph.json` is in the root of the repository, simply specify `langgraph.json`.\n2. Determine if the deployment should be `Shareable through LangGraph Studio`.\n      1. If unchecked, the deployment will only be accessible with a valid LangSmith API key for the workspace.\n      2. If checked, the deployment will be accessible through LangGraph Studio to any LangSmith user. A direct URL to LangGraph Studio for the deployment will be provided to share with other LangSmith users.\n3. Specify `Environment Variables` and secrets. Existing secrets and environment variables are prepopulated. See the [Environment Variables reference](https://langchain-ai.github.io/langgraph/cloud/reference/env_var/) to configure additional variables for the revision.\n      1. Add new secrets or environment variables.\n      2. Remove existing secrets or environment variables.\n      3. Update the value of existing secrets or environment variables.\n5. Select `Submit`. After a few seconds, the `New Revision` modal will close and the new revision will be queued for deployment.\n\n## View Build and Deployment Logs [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/\\#view-build-and-deployment-logs \"Permanent link\")\n\nBuild and deployment logs are available for each revision.\n\nStarting from the `Deployment` view...\n\n1. Select the desired revision from the `Revisions` table. A panel slides open from the right-hand side and the `Build` tab is selected by default, which displays build logs for the revision.\n2. In the panel, select the `Deploy` tab to view deployment logs for the revision.\n3. Within the `Deploy` tab, adjust the date/time range picker as needed. By default, the date/time range picker is set to the `Last 15 minutes`.\n\n## Interrupt Revision [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/\\#interrupt-revision \"Permanent link\")\n\nInterrupting a revision will stop deployment of the revision.\n\nUndefined Behavior\n\nInterrupted revisions have undefined behavior. This is only useful if you need to deploy a new revision and you already have a revision \"stuck\" in progress. In the future, this feature may be removed.\n\nStarting from the `Deployment` view...\n\n1. Select the menu icon (three dots) on the right-hand side of the row for the desired revision from the `Revisions` table.\n2. Select `Interrupt` from the menu.\n3. A modal will appear. Review the confirmation message. Select `Interrupt revision`.\n\n## Delete Deployment [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/\\#delete-deployment \"Permanent link\")\n\nStarting from the [LangSmith UI](https://smith.langchain.com/)...\n\n1. In the left-hand navigation panel, select `Deployments`. The `Deployments` view contains a list of existing LangGraph Cloud deployments.\n2. Select the menu icon (three dots) on the right-hand side of the row for the desired deployment and select `Delete`.\n3. A `Confirmation` modal will appear. Select `Delete`.\n\n## Deployment Settings [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/\\#deployment-settings \"Permanent link\")\n\nStarting from the `Deployment` view...\n\n1. In the top-right corner, select the gear icon ( `Deployment Settings`).\n2. Update the `Git Branch` to the desired branch.\n3. Check/uncheck checkbox to `Automatically update deployment on push to branch`.\n1. Branch creation/deletion and tag creation/deletion events will not trigger an update. Only pushes to an existing branch will trigger an update.\n2. Pushes in quick succession to a branch will not trigger subsequent updates. In the future, this functionality may be changed/improved.\n\n## Comments\n\nBack to top",
    "metadata": {
      "title": "Deploy to Cloud",
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  },
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/#cloud-concepts)\n\n# Cloud Concepts [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/\\#cloud-concepts \"Permanent link\")\n\nThis page describes the high-level concepts of the LangGraph Cloud deployment.\n\n## Deployment [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/\\#deployment \"Permanent link\")\n\nA deployment is an instance of a LangGraph API. A single deployment can have many [revisions](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/#revision). When a deployment is created, all of the necessary infrastructure (e.g. database, containers, secrets store) are automatically provisioned. See the [architecture diagram](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/#architecture) below for more details.\n\nSee the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/#create-new-deployment) for creating a new deployment.\n\n## Revision [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/\\#revision \"Permanent link\")\n\nA revision is an iteration of a [deployment](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/#deployment). When a new deployment is created, an initial revision is automatically created. To deploy new code changes or update environment variable configurations for a deployment, a new revision must be created. When a revision is created, a new container image is built automatically.\n\nSee the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/#create-new-revision) for creating a new revision.\n\n## Asynchronous Deployment [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/\\#asynchronous-deployment \"Permanent link\")\n\nInfrastructure for [deployments](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/#deployment) and [revisions](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/#revision) are provisioned and deployed asynchronously. They are not deployed immediately after submission. Currently, deployment can take up to several minutes.\n\n## Architecture [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/\\#architecture \"Permanent link\")\n\nSubject to Change\n\nThe LangGraph Cloud deployment architecture may change in the future.\n\nA high-level diagram of a LangGraph Cloud deployment.\n\n![diagram](https://langchain-ai.github.io/langgraph/cloud/concepts/langgraph_cloud_architecture.png)\n\n## Comments\n\nBack to top",
    "metadata": {
      "title": "Cloud Concepts",
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/cloud/concepts/cloud/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  },
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/#langgraph)\n\n# 🦜🕸️LangGraph [¶](https://langchain-ai.github.io/langgraph/\\#langgraph \"Permanent link\")\n\n![Version](https://img.shields.io/pypi/v/langgraph)[![Downloads](https://static.pepy.tech/badge/langgraph/month)](https://pepy.tech/project/langgraph)[![Open Issues](https://img.shields.io/github/issues-raw/langchain-ai/langgraph)](https://github.com/langchain-ai/langgraph/issues)[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://langchain-ai.github.io/langgraph/)\n\n⚡ Building language agents as graphs ⚡\n\nNote\n\nLooking for the JS version? Click [here](https://github.com/langchain-ai/langgraphjs) ( [JS docs](https://langchain-ai.github.io/langgraphjs/)).\n\n## Overview [¶](https://langchain-ai.github.io/langgraph/\\#overview \"Permanent link\")\n\n[LangGraph](https://langchain-ai.github.io/langgraph/) is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\nTo learn more about LangGraph, check out our first LangChain Academy course, _Introduction to LangGraph_, available for free [here](https://academy.langchain.com/courses/intro-to-langgraph).\n\n### Key Features [¶](https://langchain-ai.github.io/langgraph/\\#key-features \"Permanent link\")\n\n- **Cycles and Branching**: Implement loops and conditionals in your apps.\n- **Persistence**: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\n- **Human-in-the-Loop**: Interrupt graph execution to approve or edit next action planned by the agent.\n- **Streaming Support**: Stream outputs as they are produced by each node (including token streaming).\n- **Integration with LangChain**: LangGraph integrates seamlessly with [LangChain](https://github.com/langchain-ai/langchain/) and [LangSmith](https://docs.smith.langchain.com/) (but does not require them).\n\n## Installation [¶](https://langchain-ai.github.io/langgraph/\\#installation \"Permanent link\")\n\n```md-code__content\npip install -U langgraph\n\n```\n\n## Example [¶](https://langchain-ai.github.io/langgraph/\\#example \"Permanent link\")\n\nOne of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function.\n\nLet's take a look at a simple example of an agent that can use a search tool.\n\n```md-code__content\npip install langchain-anthropic\n\n```\n\n```md-code__content\nexport ANTHROPIC_API_KEY=sk-...\n\n```\n\nOptionally, we can set up [LangSmith](https://docs.smith.langchain.com/) for best-in-class observability.\n\n```md-code__content\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=lsv2_sk_...\n\n```\n\n```md-code__content\nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\n# Define the tools for the agent to use\n@tool\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder, but don't tell the LLM that...\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0).bind_tools(tools)\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    messages = state['messages']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the \"tools\" node\n    if last_message.tool_calls:\n        return \"tools\"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state['messages']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"tools\", 'agent')\n\n# Initialize memory to persist state between graph runs\ncheckpointer = MemorySaver()\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable.\n# Note that we're (optionally) passing the memory when compiling the graph\napp = workflow.compile(checkpointer=checkpointer)\n\n# Use the Runnable\nfinal_state = app.invoke(\n    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n    config={\"configurable\": {\"thread_id\": 42}}\n)\nfinal_state[\"messages\"][-1].content\n\n```\n\n```md-code__content\n\"Based on the search results, I can tell you that the current weather in San Francisco is:\\n\\nTemperature: 60 degrees Fahrenheit\\nConditions: Foggy\\n\\nSan Francisco is known for its microclimates and frequent fog, especially during the summer months. The temperature of 60°F (about 15.5°C) is quite typical for the city, which tends to have mild temperatures year-round. The fog, often referred to as \"Karl the Fog\" by locals, is a characteristic feature of San Francisco\\'s weather, particularly in the mornings and evenings.\\n\\nIs there anything else you\\'d like to know about the weather in San Francisco or any other location?\"\n\n```\n\nNow when we pass the same `\"thread_id\"`, the conversation context is retained via the saved state (i.e. stored list of messages)\n\n```md-code__content\nfinal_state = app.invoke(\n    {\"messages\": [HumanMessage(content=\"what about ny\")]},\n    config={\"configurable\": {\"thread_id\": 42}}\n)\nfinal_state[\"messages\"][-1].content\n\n```\n\n```md-code__content\n\"Based on the search results, I can tell you that the current weather in New York City is:\\n\\nTemperature: 90 degrees Fahrenheit (approximately 32.2 degrees Celsius)\\nConditions: Sunny\\n\\nThis weather is quite different from what we just saw in San Francisco. New York is experiencing much warmer temperatures right now. Here are a few points to note:\\n\\n1. The temperature of 90°F is quite hot, typical of summer weather in New York City.\\n2. The sunny conditions suggest clear skies, which is great for outdoor activities but also means it might feel even hotter due to direct sunlight.\\n3. This kind of weather in New York often comes with high humidity, which can make it feel even warmer than the actual temperature suggests.\\n\\nIt's interesting to see the stark contrast between San Francisco's mild, foggy weather and New York's hot, sunny conditions. This difference illustrates how varied weather can be across different parts of the United States, even on the same day.\\n\\nIs there anything else you'd like to know about the weather in New York or any other location?\"\n\n```\n\n### Step-by-step Breakdown [¶](https://langchain-ai.github.io/langgraph/\\#step-by-step-breakdown \"Permanent link\")\n\n1. Initialize the model and tools.\n\n\n\n\n\n- we use `ChatAnthropic` as our LLM. **NOTE:** we need make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI tool calling using the `.bind_tools()` method.\n- we define the tools we want to use - a search tool in our case. It is really easy to create your own tools - see documentation here on how to do that [here](https://python.langchain.com/docs/modules/agents/tools/custom_tools).\n\n\n2. Initialize graph with state.\n\n\n\n\n\n- we initialize graph ( `StateGraph`) by passing state schema (in our case `MessagesState`)\n- `MessagesState` is a prebuilt state schema that has one attribute -- a list of LangChain `Message` objects, as well as logic for merging the updates from each node into the state\n\n\n3. Define graph nodes.\n\n\n\n\n\nThere are two main nodes we need:\n\n\n\n- The `agent` node: responsible for deciding what (if any) actions to take.\n- The `tools` node that invokes tools: if the agent decides to take an action, this node will then execute that action.\n\n\n4. Define entry point and graph edges.\n\n\n\n\n\nFirst, we need to set the entry point for graph execution - `agent` node.\n\n\n\nThen we define one normal and one conditional edge. Conditional edge means that the destination depends on the contents of the graph's state ( `MessageState`). In our case, the destination is not known until the agent (LLM) decides.\n\n\n\n- Conditional edge: after the agent is called, we should either:\n  - a. Run tools if the agent said to take an action, OR\n  - b. Finish (respond to the user) if the agent did not ask to run tools\n- Normal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next\n\n\n5. Compile the graph.\n\n\n\n\n\n- When we compile the graph, we turn it into a LangChain [Runnable](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface), which automatically enables calling `.invoke()`, `.stream()` and `.batch()` with your inputs\n- We can also optionally pass checkpointer object for persisting state between graph runs, and enabling memory, human-in-the-loop workflows, time travel and more. In our case we use `MemorySaver` \\- a simple in-memory checkpointer\n\n\n6. Execute the graph.\n\n\n\n\n\n1. LangGraph adds the input message to the internal state, then passes the state to the entrypoint node, `\"agent\"`.\n2. The `\"agent\"` node executes, invoking the chat model.\n3. The chat model returns an `AIMessage`. LangGraph adds this to the state.\n4. Graph cycles the following steps until there are no more `tool_calls` on `AIMessage`:\n   - If `AIMessage` has `tool_calls`, `\"tools\"` node executes\n   - The `\"agent\"` node executes again and returns `AIMessage`\n5. Execution progresses to the special `END` value and outputs the final state.\nAnd as a result, we get a list of all our chat messages as output.\n\n\n\n## Documentation [¶](https://langchain-ai.github.io/langgraph/\\#documentation \"Permanent link\")\n\n- [Tutorials](https://langchain-ai.github.io/langgraph/tutorials/): Learn to build with LangGraph through guided examples.\n- [How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/): Accomplish specific things within LangGraph, from streaming, to adding memory & persistence, to common design patterns (branching, subgraphs, etc.), these are the place to go if you want to copy and run a specific code snippet.\n- [Conceptual Guides](https://langchain-ai.github.io/langgraph/concepts/high_level/): In-depth explanations of the key concepts and principles behind LangGraph, such as nodes, edges, state and more.\n- [API Reference](https://langchain-ai.github.io/langgraph/reference/graphs/): Review important classes and methods, simple examples of how to use the graph and checkpointing APIs, higher-level prebuilt components and more.\n- [Cloud (beta)](https://langchain-ai.github.io/langgraph/cloud/): With one click, deploy LangGraph applications to LangGraph Cloud.\n\n## Contributing [¶](https://langchain-ai.github.io/langgraph/\\#contributing \"Permanent link\")\n\nFor more information on how to contribute, see [here](https://github.com/langchain-ai/langgraph/blob/main/CONTRIBUTING.md).\n\nBack to top",
    "metadata": {
      "title": "Home",
      "sitemap": {
        "lastmod": "2024-10-05"
      },
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  },
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/#rebuild-graph-at-runtime)\n\n# Rebuild Graph at Runtime [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/\\#rebuild-graph-at-runtime \"Permanent link\")\n\nYou might need to rebuild your graph with a different configuration for a new run. For example, you might need to use a different graph state or graph structure depending on the config. This guide shows how you can do this.\n\nNote\n\nIn most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it\n\n## Prerequisites [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/\\#prerequisites \"Permanent link\")\n\nMake sure to check out [this how-to guide](https://langchain-ai.github.io/langgraph/cloud/deployment/setup/) on setting up your app for deployment first.\n\n## Define graphs [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/\\#define-graphs \"Permanent link\")\n\nLet's say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:\n\n```md-code__content\nmy-app/\n|-- requirements.txt\n|-- .env\n|-- openai_agent.py     # code for your graph\n\n```\n\nwhere the graph is defined in `openai_agent.py`.\n\n### No rebuild [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/\\#no-rebuild \"Permanent link\")\n\nIn the standard LangGraph API configuration, the server uses the compiled graph instance that's defined at the top level of `openai_agent.py`, which looks like the following:\n\n```md-code__content\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, START, MessageGraph\n\nmodel = ChatOpenAI(temperature=0)\n\ngraph_workflow = MessageGraph()\n\ngraph_workflow.add_node(\"agent\", model)\ngraph_workflow.add_edge(\"agent\", END)\ngraph_workflow.add_edge(START, \"agent\")\n\nagent = graph_workflow.compile()\n\n```\n\nTo make the server aware of your graph, you need to specify a path to the variable that contains the `CompiledStateGraph` instance in your LangGraph API configuration ( `langgraph.json`), e.g.:\n\n```md-code__content\n{\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"openai_agent\": \"./openai_agent.py:agent\",\n    },\n    \"env\": \"./.env\"\n}\n\n```\n\n### Rebuild [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/\\#rebuild \"Permanent link\")\n\nTo make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a _function_ that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:\n\n```md-code__content\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, START, MessageGraph\nfrom langgraph.graph.state import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import BaseMessage\nfrom langchain_core.runnables import RunnableConfig\n\nclass State(TypedDict):\n    messages: Annotated[list[BaseMessage], add_messages]\n\nmodel = ChatOpenAI(temperature=0)\n\ndef make_default_graph():\n    \"\"\"Make a simple LLM agent\"\"\"\n    graph_workflow = StateGraph(State)\n    def call_model(state):\n        return {\"messages\": [model.invoke(state[\"messages\"])]}\n\n    graph_workflow.add_node(\"agent\", call_model)\n    graph_workflow.add_edge(\"agent\", END)\n    graph_workflow.add_edge(START, \"agent\")\n\n    agent = graph_workflow.compile()\n    return agent\n\ndef make_alternative_graph():\n    \"\"\"Make a tool-calling agent\"\"\"\n\n    @tool\n    def add(a: float, b: float):\n        \"\"\"Adds two numbers.\"\"\"\n        return a + b\n\n    tool_node = ToolNode([add])\n    model_with_tools = model.bind_tools([add])\n    def call_model(state):\n        return {\"messages\": [model_with_tools.invoke(state[\"messages\"])]}\n\n    def should_continue(state: State):\n        if state[\"messages\"][-1].tool_calls:\n            return \"tools\"\n        else:\n            return END\n\n    graph_workflow = StateGraph(State)\n\n    graph_workflow.add_node(\"agent\", call_model)\n    graph_workflow.add_node(\"tools\", tool_node)\n    graph_workflow.add_edge(\"tools\", \"agent\")\n    graph_workflow.add_edge(START, \"agent\")\n    graph_workflow.add_conditional_edges(\"agent\", should_continue)\n\n    agent = graph_workflow.compile()\n    return agent\n\n# this is the graph making function that will decide which graph to\n# build based on the provided config\ndef make_graph(config: RunnableConfig):\n    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n    # route to different graph state / structure based on the user ID\n    if user_id == \"1\":\n        return make_default_graph()\n    else:\n        return make_alternative_graph()\n\n```\n\nFinally, you need to specify the path to your graph-making function ( `make_graph`) in `langgraph.json`:\n\n```md-code__content\n{\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"openai_agent\": \"./openai_agent.py:make_graph\",\n    },\n    \"env\": \"./.env\"\n}\n\n```\n\nSee more info on LangGraph API configuration file [here](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file)\n\n## Comments\n\nBack to top",
    "metadata": {
      "title": "Rebuild Graph at Runtime",
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  },
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#api-concepts)\n\n# API Concepts [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#api-concepts \"Permanent link\")\n\nThis page describes the high-level concepts of the LangGraph Cloud API. The conceptual guide of LangGraph (Python library) is [here](https://langchain-ai.github.io/langgraph/concepts/high_level/).\n\n## Data Models [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#data-models \"Permanent link\")\n\nThe LangGraph Cloud API consists of a few core data models: [Assistants](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#assistants), [Threads](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#threads), [Runs](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#runs), and [Cron Jobs](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#cron-jobs).\n\n### Assistants [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#assistants \"Permanent link\")\n\nWhen building agents, it is fairly common to make rapid changes that _do not_ alter the graph logic. For example, simply changing prompts or the LLM selection can have significant impacts on the behavior of the agents. Assistants offer an easy way to make and save these types of changes to agent configuration. This can have at least two use-cases:\n\n- Assistants give developers a quick and easy way to modify and version graph version for experimentation.\n- Assistants can be modified via LangGraph Studio, offering a no-code way to configure agents (e.g., for business users).\n\n#### Configuring Assistants [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#configuring-assistants \"Permanent link\")\n\nIn practice, an assistant is just an _instance_ of a graph with a specific configuration. Because of this, multiple assistants can reference the same graph but can contain different configurations, such as prompts, models, and other graph configuration options. The LangGraph Cloud API provides several endpoints for creating and managing assistants. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/assistantscreate) and [this how-to](https://langchain-ai.github.io/langgraph/cloud/how-tos/configuration_cloud/) for more details on how to create assistants.\n\n#### Versioning Assistants [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#versioning-assistants \"Permanent link\")\n\n![assistant versions](https://langchain-ai.github.io/langgraph/cloud/concepts/assistant_version.png)\n\nOnce you've created an assistant, you can save and version it to track changes to the configuration over time. You can think about this at three levels:\n\n1) The graph lays out the general agent application logic\n2) The agent configuration options represent parameters that can be changed\n3) Assistant versions save and track specific settings of the agent configuration options\n\nFor example, if you have an agent that helps for planning trips, you can create a new assistant _for each user_ that passes specific user preferences (e.g., desired airline and car service). As each user interacts with their own assistant, assistant versions can be saved that track the specific desires of the user. Read [this how-to](https://langchain-ai.github.io/langgraph/cloud/how-tos/assistant_versioning/) to learn how you can use assistant versioning through both the [Studio](https://langchain-ai.github.io/langgraph/cloud/how-tos/#langgraph-studio) and the SDK.\n\n### Threads [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#threads \"Permanent link\")\n\nA thread contains the accumulated state of a group of runs. If a run is executed on a thread, then the [state](https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/#state) of the underlying graph of the assistant will be persisted to the thread. A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run.\n\nThe state of a thread at a particular point in time is called a checkpoint.\n\nFor more on threads and checkpoints, see this section of the [LangGraph conceptual guide](https://langchain-ai.github.io/langgraph/concepts/low_level/#persistence).\n\nThe LangGraph Cloud API provides several endpoints for creating and managing threads and thread state. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/threadscreate) for more details.\n\n### Runs [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#runs \"Permanent link\")\n\nA run is an invocation of an assistant. Each run may have its own input, configuration, and metadata, which may affect execution and output of the underlying graph. A run can optionally be executed on a thread.\n\nThe LangGraph Cloud API provides several endpoints for creating and managing runs. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/runscreate) for more details.\n\n### Cron Jobs [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#cron-jobs \"Permanent link\")\n\nIt's often useful to run graphs on some schedule. LangGraph Cloud supports cron jobs, which run on a user defined schedule. The user specifies a schedule, an assistant, and some input. After than, on the specified schedule LangGraph cloud will:\n\n- Create a new thread with the specified assistant\n- Send the specified input to that thread\n\nNote that this sends the same input to the thread every time. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/cron_jobs/) for creating cron jobs.\n\nThe LangGraph Cloud API provides several endpoints for creating and managing cron jobs. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/runscreate/POST/threads/{thread_id}/runs/crons) for more details.\n\n## Features [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#features \"Permanent link\")\n\nThe LangGraph Cloud API offers several features to support complex agent architectures.\n\n### Streaming [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#streaming \"Permanent link\")\n\nStreaming is critical for making LLM applications feel responsive to end users. When creating a streaming run, the streaming mode determines what data is streamed back to the API client. The LangGraph Cloud API supports five streaming modes.\n\n- `values`: Stream the full state of the graph after each [super-step](https://langchain-ai.github.io/langgraph/concepts/low_level/#graphs) is executed. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/) for streaming values.\n- `messages`: Stream complete messages (at the end of node execution) as well as tokens for any messages generated inside a node. This mode is primarily meant for powering chat applications. This is only an option if your graph contains a `messages` key. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) for streaming messages.\n- `updates`: Streams updates to the state of the graph after each node is executed. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_updates/) for streaming updates.\n- `events`: Stream all events (including the state of the graph) that occur during graph execution. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_events/) for streaming events. This can be used to do token-by-token streaming for LLMs.\n- `debug`: Stream debug events throughout graph execution. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_debug/) for streaming debug events.\n\nYou can also specify multiple streaming modes at the same time. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_multiple/) for configuring multiple streaming modes at the same time.\n\nSee the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/runscreate/POST/threads/{thread_id}/runs/stream) for how to create streaming runs.\n\nStreaming modes `values`, `updates`, and `debug` are very similar to modes available in the LangGraph library - for a deeper conceptual explanation of those, you can see the LangGraph library documentation [here](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n\nStreaming mode `events` is the same as using `.astream_events` in the LangGraph library - for a deeper conceptual explanation of this, you can see the LangGraph library documentation [here](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n\n#### `mode=\"messages\"` [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#modemessages \"Permanent link\")\n\nStreaming mode `messages` is a new streaming mode, currently only available in the API. What does this mode enable?\n\nThis mode is focused on streaming back messages. It currently assumes that you have a `messages` key in your graph that is a list of messages. Assuming we have a simple react agent deployed, what does this stream look like?\n\nAll events emitted have two attributes:\n\n- `event`: This is the name of the event\n- `data`: This is data associated with the event\n\nLet's run it on a question that should trigger a tool call:\n\n```md-code__content\nthread = await client.threads.create()\ninput = {\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]}\n\nevents = []\nasync for event in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id=\"agent\",  # This may need to change depending on the graph you deployed\n    input=input,\n    stream_mode=\"messages\",\n):\n    print(event.event)\n\n```\n\n```md-code__content\nmetadata\nmessages/complete\nmessages/metadata\nmessages/partial\n...\nmessages/partial\nmessages/complete\nmessages/complete\nmessages/metadata\nmessages/partial\n...\nmessages/partial\nmessages/complete\nend\n\n```\n\nWe first get some `metadata` \\- this is metadata about the run.\n\n```md-code__content\nStreamPart(event='metadata', data={'run_id': '1ef657cf-ae55-6f65-97d4-f4ed1dbdabc6'})\n\n```\n\nWe then get a `messages/complete` event - this a fully formed message getting emitted. In this case,\nthis was the just the input message we sent in.\n\n```md-code__content\nStreamPart(event='messages/complete', data=[{'content': 'hi!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '833c09a3-bb19-46c9-81d9-1e5954ec5f92', 'example': False}])\n\n```\n\nWe then get a `messages/metadata` \\- this is just letting us know that a new message is starting.\n\n```md-code__content\nStreamPart(event='messages/metadata', data={'run-985c0f14-9f43-40d4-a505-4637fc58e333': {'metadata': {'created_by': 'system', 'run_id': '1ef657de-7594-66df-8eb2-31518e4a1ee2', 'graph_id': 'agent', 'thread_id': 'c178eab5-e293-423c-8e7d-1d113ffe7cd9', 'model_name': 'openai', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0, 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}})\n\n```\n\nWe then get a BUNCH of `messages/partial` events - these are the individual tokens from the LLM! In the case below, we can see the START of a tool call.\n\n```md-code__content\nStreamPart(event='messages/partial', data=[{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_w8Hr8dHGuZCPgRfd5FqRBArs', 'function': {'arguments': '', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-985c0f14-9f43-40d4-a505-4637fc58e333', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [{'name': 'tavily_search_results_json', 'args': '', 'id': 'call_w8Hr8dHGuZCPgRfd5FqRBArs', 'error': None}], 'usage_metadata': None}])\n\n```\n\nAfter that, we get a `messages/complete` event - this is the AIMessage finishing. It's now a complete tool call:\n\n```md-code__content\nStreamPart(event='messages/complete', data=[{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_w8Hr8dHGuZCPgRfd5FqRBArs', 'function': {'arguments': '{\"query\":\"current weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-985c0f14-9f43-40d4-a505-4637fc58e333', 'example': False, 'tool_calls': [{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_w8Hr8dHGuZCPgRfd5FqRBArs'}], 'invalid_tool_calls': [], 'usage_metadata': None}])\n\n```\n\nAfter that, we get ANOTHER `messages/complete` event. This is a tool message - our agent has called a tool, gotten a response, and now inserting it into the state in the form of a tool message.\n\n```md-code__content\nStreamPart(event='messages/complete', data=[{'content': '[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1724877689, \\'localtime\\': \\'2024-08-28 13:41\\'}, \\'current\\': {\\'last_updated_epoch\\': 1724877000, \\'last_updated\\': \\'2024-08-28 13:30\\', \\'temp_c\\': 23.3, \\'temp_f\\': 73.9, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 15.0, \\'wind_kph\\': 24.1, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1014.0, \\'pressure_in\\': 29.93, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 57, \\'cloud\\': 25, \\'feelslike_c\\': 25.0, \\'feelslike_f\\': 77.1, \\'windchill_c\\': 20.9, \\'windchill_f\\': 69.6, \\'heatindex_c\\': 23.3, \\'heatindex_f\\': 74.0, \\'dewpoint_c\\': 12.9, \\'dewpoint_f\\': 55.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 6.0, \\'gust_mph\\': 19.5, \\'gust_kph\\': 31.3}}\"}]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'tavily_search_results_json', 'id': '0112eba5-7660-4375-9f24-c7a1d6777b97', 'tool_call_id': 'call_w8Hr8dHGuZCPgRfd5FqRBArs'}])\n\n```\n\nAfter that, we see the agent doing another LLM call and streaming back a response. We then get an `end` event:\n\n```md-code__content\nStreamPart(event='end', data=None)\n\n```\n\nAnd that's it! This is more focused streaming mode specifically focused on streaming back messages. See this [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) for more information.\n\n### Human-in-the-Loop [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#human-in-the-loop \"Permanent link\")\n\nThere are many occasions where the graph cannot run completely autonomously. For instance, the user might need to input some additional arguments to a function call, or select the next edge for the graph to continue on. In these instances, we need to insert some human in the loop interaction, which you can learn about in the [human in the loop how-tos](https://langchain-ai.github.io/langgraph/cloud/how-tos/#human-in-the-loop).\n\n### Double Texting [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#double-texting \"Permanent link\")\n\nMany times users might interact with your graph in unintended ways. For instance, a user may send one message and before the graph has finished running send a second message. To solve this issue of \"double-texting\" (i.e. prompting the graph a second time before the first run has finished), LangGraph has provided four different solutions, all of which are covered in the [Double Texting how-tos](https://langchain-ai.github.io/langgraph/cloud/how-tos/#double-texting). These options are:\n\n- `reject`: This is the simplest option, this just rejects any follow up runs and does not allow double texting. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/reject_concurrent/) for configuring the reject double text option.\n- `enqueue`: This is a relatively simple option which continues the first run until it completes the whole run, then sends the new input as a separate run. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/enqueue_concurrent/) for configuring the enqueue double text option.\n- `interrupt`: This option interrupts the current execution but saves all the work done up until that point. It then inserts the user input and continues from there. If you enable this option, your graph should be able to handle weird edge cases that may arise. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/interrupt_concurrent/) for configuring the interrupt double text option.\n- `rollback`: This option rolls back all work done up until that point. It then sends the user input in, basically as if it just followed the original run input. See the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/rollback_concurrent/) for configuring the rollback double text option.\n\n### Stateless Runs [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#stateless-runs \"Permanent link\")\n\nAll runs use the built-in checkpointer to store checkpoints for runs. However, it can often be useful to just kick off a run without worrying about explicitly creating a thread and without wanting to keep those checkpointers around. Stateless runs allow you to do this by exposing an endpoint that:\n\n- Takes in user input\n- Under the hood, creates a thread\n- Runs the agent but skips all checkpointing steps\n- Cleans up the thread afterwards\n\nStateless runs are still retried as regular retries are per node, while everything still in memory, so doesn't use checkpoints.\n\nThe only difference is in stateless background runs, if the task worker dies halfway (not because the run itself failed, for some external reason) then the whole run will be retried like any background run, but\n\n- whereas a stateful background run would retry from the last successful checkpoint\n- a stateless background run would retry from the beginning\n\nSee the [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/stateless_runs/) for creating stateless runs.\n\n### Webhooks [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#webhooks \"Permanent link\")\n\nFor all types of runs, langgraph cloud supports completion webhooks. When you create the run you can pass a webhook URL to be called when the completes (successfully or not). This is especially useful for background runs and cron jobs, as the webhook can give you an indication the run has completed and you can perform further actions for your appilcation.\n\nSee this [how-to guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/webhooks/) to learn about how to use webhooks with LangGraph Cloud.\n\n## Deployment [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#deployment \"Permanent link\")\n\nThe LangGraph Cloud offers several features to support secure and robost deployments.\n\n### Authentication [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#authentication \"Permanent link\")\n\nLangGraph applications deployed to LangGraph Cloud are automatically configured with LangSmith authentication. In order to call the API, a valid [LangSmith API key](https://docs.smith.langchain.com/how_to_guides/setup/create_account_api_key#api-keys) is required.\n\n### Local Testing [¶](https://langchain-ai.github.io/langgraph/cloud/concepts/api/\\#local-testing \"Permanent link\")\n\nBefore deploying your app in production to LangGraph Cloud, you may wish to test out your graph locally in order to ensure that everything is running as expected. Luckily, LangGraph makes this easy for you through use of the LangGraph CLI. Read more in this [how-to guide](https://langchain-ai.github.io/langgraph/cloud/deployment/test_locally/) or look at the [CLI reference](https://langchain-ai.github.io/langgraph/cloud/reference/cli/) to learn more.\n\n## Comments\n\nBack to top",
    "metadata": {
      "title": "API Concepts",
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/cloud/concepts/api/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  },
  {
    "markdown": "[Skip to content](https://langchain-ai.github.io/langgraph/cloud/deployment/setup/#how-to-set-up-a-langgraph-application-for-deployment)\n\n# How to Set Up a LangGraph Application for Deployment [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup/\\#how-to-set-up-a-langgraph-application-for-deployment \"Permanent link\")\n\nA LangGraph application must be configured with a [LangGraph API configuration file](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) in order to be deployed to LangGraph Cloud (or to be self-hosted). This how-to guide discusses the basic steps to setup a LangGraph application for deployment using `requirements.txt` to specify project dependencies.\n\nThis walkthrough is based on [this repository](https://github.com/langchain-ai/langgraph-example), which you can play around with to learn more about how to setup your LangGraph application for deployment.\n\nSetup with pyproject.toml\n\nIf you prefer using poetry for dependency management, check out [this how-to guide](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_pyproject/) on using `pyproject.toml` for LangGraph Cloud.\n\nSetup with a Monorepo\n\nIf you are interested in deploying a graph located inside a monorepo, take a look at [this](https://github.com/langchain-ai/langgraph-example-monorepo) repository for an example of how to do so.\n\nThe final repo structure will look something like this:\n\n```md-code__content\nmy-app/\n├── my_agent # all project code lies within here\n│   ├── utils # utilities for your graph\n│   │   ├── __init__.py\n│   │   ├── tools.py # tools for your graph\n│   │   ├── nodes.py # node functions for you graph\n│   │   └── state.py # state definition of your graph\n│   ├── requirements.txt # package dependencies\n│   ├── __init__.py\n│   └── agent.py # code for constructing your graph\n├── .env # environment variables\n└── langgraph.json # configuration file for LangGraph\n\n```\n\nAfter each step, an example file directory is provided to demonstrate how code can be organized.\n\n## Specify Dependencies [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup/\\#specify-dependencies \"Permanent link\")\n\nDependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [LangGraph API configuration file](https://langchain-ai.github.io/langgraph/cloud/deployment/setup/#create-langgraph-api-config).\n\nThe dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:\n\n```md-code__content\nlanggraph>=0.2.30,<0.3.0\nlanggraph-checkpoint>=1.0.14\nlangchain-core>=0.2.38,<0.4.0\nlangsmith>=0.1.63\norjson>=3.9.7\nhttpx>=0.25.0\ntenacity>=8.0.0\nuvicorn>=0.26.0\nsse-starlette>=2.1.0\nuvloop>=0.18.0\nhttptools>=0.5.0\njsonschema-rs>=0.16.3\ncroniter>=1.0.1\nstructlog>=23.1.0\nredis>=5.0.0,<6.0.0\n\n```\n\nExample `requirements.txt` file:\n\n```md-code__content\nlanggraph\nlangchain_anthropic\ntavily-python\nlangchain_community\nlangchain_openai\n\n```\n\nExample file directory:\n\n```md-code__content\nmy-app/\n├── my_agent # all project code lies within here\n│   └── requirements.txt # package dependencies\n\n```\n\n## Specify Environment Variables [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup/\\#specify-environment-variables \"Permanent link\")\n\nEnvironment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](https://langchain-ai.github.io/langgraph/cloud/reference/env_var/) to configure additional variables for a deployment.\n\nExample `.env` file:\n\n```md-code__content\nMY_ENV_VAR_1=foo\nMY_ENV_VAR_2=bar\nOPENAI_API_KEY=key\n\n```\n\nExample file directory:\n\n```md-code__content\nmy-app/\n├── my_agent # all project code lies within here\n│   └── requirements.txt # package dependencies\n└── .env # environment variables\n\n```\n\n## Define Graphs [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup/\\#define-graphs \"Permanent link\")\n\nImplement your graphs! Graphs can be defined in a single file or multiple files. Make note of the variable names of each [CompiledGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.graph.CompiledGraph) to be included in the LangGraph application. The variable names will be used later when creating the [LangGraph API configuration file](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file).\n\nExample `agent.py` file, which shows how to import from other modules you define (code for the modules is not shown here, please see [this repo](https://github.com/langchain-ai/langgraph-example) to see their implementation):\n\n```md-code__content\n# my_agent/agent.py\nfrom typing import Literal\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, END, START\nfrom my_agent.utils.nodes import call_model, should_continue, tool_node # import nodes\nfrom my_agent.utils.state import AgentState # import state\n\n# Define the config\nclass GraphConfig(TypedDict):\n    model_name: Literal[\"anthropic\", \"openai\"]\n\nworkflow = StateGraph(AgentState, config_schema=GraphConfig)\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", tool_node)\nworkflow.add_edge(START, \"agent\")\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n    {\n        \"continue\": \"action\",\n        \"end\": END,\n    },\n)\nworkflow.add_edge(\"action\", \"agent\")\n\ngraph = workflow.compile()\n\n```\n\nAssign `CompiledGraph` to Variable\n\nThe build process for LangGraph Cloud requires that the `CompiledGraph` object be assigned to a variable at the top-level of a Python module (alternatively, you can provide [a function that creates a graph](https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/)).\n\nExample file directory:\n\n```md-code__content\nmy-app/\n├── my_agent # all project code lies within here\n│   ├── utils # utilities for your graph\n│   │   ├── __init__.py\n│   │   ├── tools.py # tools for your graph\n│   │   ├── nodes.py # node functions for you graph\n│   │   └── state.py # state definition of your graph\n│   ├── requirements.txt # package dependencies\n│   ├── __init__.py\n│   └── agent.py # code for constructing your graph\n└── .env # environment variables\n\n```\n\n## Create LangGraph API Config [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup/\\#create-langgraph-api-config \"Permanent link\")\n\nCreate a [LangGraph API configuration file](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) called `langgraph.json`. See the [LangGraph CLI reference](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) for detailed explanations of each key in the JSON object of the configuration file.\n\nExample `langgraph.json` file:\n\n```md-code__content\n{\n  \"dependencies\": [\"./my_agent\"],\n  \"graphs\": {\n    \"agent\": \"./my_agent/agent.py:graph\"\n  },\n  \"env\": \".env\"\n}\n\n```\n\nNote that the variable name of the `CompiledGraph` appears at the end of the value of each subkey in the top-level `graphs` key (i.e. `:<variable_name>`).\n\nConfiguration Location\n\nThe LangGraph API configuration file must be placed in a directory that is at the same level or higher than the Python files that contain compiled graphs and associated dependencies.\n\nExample file directory:\n\n```md-code__content\nmy-app/\n├── my_agent # all project code lies within here\n│   ├── utils # utilities for your graph\n│   │   ├── __init__.py\n│   │   ├── tools.py # tools for your graph\n│   │   ├── nodes.py # node functions for you graph\n│   │   └── state.py # state definition of your graph\n│   ├── requirements.txt # package dependencies\n│   ├── __init__.py\n│   └── agent.py # code for constructing your graph\n├── .env # environment variables\n└── langgraph.json # configuration file for LangGraph\n\n```\n\n## Next [¶](https://langchain-ai.github.io/langgraph/cloud/deployment/setup/\\#next \"Permanent link\")\n\nAfter you setup your project and place it in a github repo, it's time to [deploy your app](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/).\n\n## Comments\n\nBack to top",
    "metadata": {
      "title": "Setup App",
      "language": "en",
      "sourceURL": "https://langchain-ai.github.io/langgraph/cloud/deployment/setup/",
      "description": "Build language agents as graphs",
      "ogLocaleAlternate": [],
      "statusCode": 200
    }
  }
]